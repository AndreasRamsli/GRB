{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "honey-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-plastic",
   "metadata": {},
   "source": [
    "## Objective:\n",
    "-Import the datasets. Still need the extra dataset. Includes triggers up to dec. 21\n",
    "\n",
    "-datetime helpful docs: https://www.w3schools.com/python/python_datetime.asp\n",
    "\n",
    "-Create algorithm that searches for matches that are +/- 10 second apart and remove the rest (maybe two seconds)\n",
    "\n",
    "-First match found at 2018, 7, 11, 17, 2, 4\n",
    "\n",
    "-Complete match list found for trigB --> trigB_match\n",
    "\n",
    "//TODO: \n",
    "\n",
    "- Update list up until 2022\n",
    "- Wrap functions around the algorithms so that the same can be done for trigC etc.\n",
    "- Create dataframe with the result\n",
    "-Access the relevant files found in trigB_match list\n",
    "-Do spectral analysis on the data (counts/bin)\n",
    "\n",
    "\n",
    "### Notes from stackoverflow:\n",
    "np.where() implementation:https://numpy.org/doc/stable/reference/generated/numpy.where.html , https://stackoverflow.com/questions/35714902/find-location-of-pair-of-elements-in-two-arrays-in-numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-steal",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "harmful-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to data\n",
    "IPN_path = \"/Home/siv30/wad005/master/GRB/IPN\"\n",
    "ICECUBE_path = \"/Home/siv30/wad005/master/GRB/IPN\"\n",
    "ASIM_path = \"/Home/siv30/wad005/master/GRB/ASIM\"\n",
    "\n",
    "#Path to export data\n",
    "IPN_match_path = \"/Home/siv30/wad005/master/GRB/matches_IPN\"\n",
    "ICECUBE_match_path = \"/Home/siv30/wad005/master/GRB/matches_ICECUBE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "lucky-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing and redefining the dataframes\n",
    "#Not loading the trigger list for 2021 just yet\n",
    "\n",
    "#Old IPN dataset\n",
    "ipn_old_data = pd.read_csv(\"{}/trigIPN.csv\".format(IPN_path), sep=\"|\")\n",
    "ipn_old_df = pd.DataFrame(ipn_old_data)\n",
    "ipn_old_df.drop(columns= ['Unnamed: 0', 'Unnamed: 2'], axis=1, inplace=True) #dropping unwanted columns\n",
    "ipn_old_df.rename(columns={ipn_old_df.columns[0]:\"datetime\"}, inplace = True)\n",
    "\n",
    "#New IPN dataset\n",
    "ipn_new_data = pd.read_csv(\"{}/icecube.txt\".format(ICECUBE_path),sep = \"\\s+|\\t+|\\s+\\t+|\\t+\\s+\", engine=\"python\")\n",
    "ipn_new_df = pd.DataFrame(ipn_new_data)\n",
    "#Dropping unwanted columns\n",
    "ipn_new_df = ipn_new_df.drop([\"GRB_name_Fermi\",'ra(°J2000)', 'decl(°J2000)',\n",
    "       'pos_error(°1-sigma)', 'T90(s)', 'T90_error(s)', 'T90_start(UTC)',\n",
    "       'fluence(erg/cm^2)', 'fluence_error(erg/cm^2)', 'redshift', 'T100(s)',\n",
    "       'GBM_located', 'mjd(T0day)'], 1)\n",
    "\n",
    "#HED \n",
    "trigB_data = pd.read_csv(\"{}/trigB.txt\".format(ASIM_path), sep = \"\\s+|\\t+|\\s+\\t+|\\t+\\s+\", engine=\"python\")\n",
    "trigB = pd.DataFrame(trigB_data)\n",
    "trigB.drop(columns=[\"######\"], inplace=True) #dropping unwanted columns\n",
    "trigB.rename(columns={\"yyyy-MMM-dd\":\"date\",\"HH:mm:ss.SSSSSS\": \"time\", \"Corr\":\"corr\"}, inplace=True) #renaming columns\n",
    "\n",
    "trigB_21_data = pd.read_csv(\"{}/trigB_2021.txt\".format(ASIM_path), sep = \"\\s+|\\t+|\\s+\\t+|\\t+\\s+\", engine=\"python\")\n",
    "trigB_21 = pd.DataFrame(trigB_21_data)\n",
    "trigB_21.drop(columns=[\"######\"], inplace=True)\n",
    "trigB_21.rename(columns={\"yyyy-MMM-dd\":\"date\",\"HH:mm:ss.SSSSSS\": \"time\" ,\"Corr\":\"corr\"}, inplace=True)\n",
    "\n",
    "#LED\n",
    "trigC_data = pd.read_csv(\"{}/trigC.txt\".format(ASIM_path), sep = \"\\s+|\\t+|\\s+\\t+|\\t+\\s+\", engine=\"python\")\n",
    "trigC = pd.DataFrame(trigC_data)\n",
    "trigC.drop(columns=[\"######\"], inplace=True)\n",
    "trigC.rename(columns={\"yyyy-MMM-dd\":\"date\",\"HH:mm:ss.SSSSSS\": \"time\" ,\"Corr\":\"corr\"}, inplace=True)\n",
    "\n",
    "trigC_21_data = pd.read_csv(\"{}/trigC_2021.txt\".format(ASIM_path), sep = \"\\s+|\\t+|\\s+\\t+|\\t+\\s+\", engine=\"python\")\n",
    "trigC_21 = pd.DataFrame(trigC_21_data)\n",
    "trigC_21.drop(columns=[\"######\"], inplace=True)\n",
    "trigC_21.rename(columns={\"yyyy-MMM-dd\":\"date\",\"HH:mm:ss.SSSSSS\": \"time\", 'Corr':\"corr\" }, inplace=True)\n",
    "\n",
    "#MMIA (visible)\n",
    "trigM_data = pd.read_csv(\"{}/trigM.txt\".format(ASIM_path), sep = \"\\s+|\\t+|\\s+\\t+|\\t+\\s+\", engine=\"python\")\n",
    "trigM = pd.DataFrame(trigM_data)\n",
    "trigM.drop(columns=[\"######\"], inplace=True)\n",
    "trigM.rename(columns={\"yyyy-MMM-dd\":\"date\",\"HH:mm:ss.SSSSSS\": \"time\", \"Corr\":\"corr\" }, inplace=True)\n",
    "\n",
    "trigM_21_data = pd.read_csv(\"{}/trigM_2021.txt\".format(ASIM_path), sep = \"\\s+|\\t+|\\s+\\t+|\\t+\\s+\", engine=\"python\")\n",
    "trigM_21 = pd.DataFrame(trigM_21_data)\n",
    "trigM_21.drop(columns=[\"######\"], inplace=True)\n",
    "trigM_21.rename(columns={\"yyyy-MMM-dd\":\"date\",\"HH:mm:ss.SSSSSS\": \"time\", \"Corr\":\"corr\" }, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-perception",
   "metadata": {},
   "source": [
    "## Concatenating ASIM dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "engaged-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_21(l):\n",
    "    \"\"\"\n",
    "    Function for finding the indecies to slice. Assuming l is sorted by date from first to last.\n",
    "    Finding index of first entry of '2021-Jan-01'.\n",
    "    \n",
    "    Parameters:\n",
    "    -----\n",
    "    l <array>\n",
    "    \n",
    "    Returns:\n",
    "    -----\n",
    "    Returning index for the first entry of '2021-Jan-01' \"\"\"\n",
    "    \n",
    "    \n",
    "    i = l.tolist().index('2021-Jan-01')\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "chubby-pierre",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[110410, 79065, 237671]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice_index = []\n",
    "for df in [trigB,trigC,trigM]:\n",
    "    temp_date_list = df[\"date\"].values\n",
    "    slice_index.append(find_index_21(temp_date_list))\n",
    "slice_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "imported-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trig*_20 contains rows to the end of 2020\n",
    "#trig*_master contains rows to the end of 2021\n",
    "trigB_20 = trigB.drop(np.arange(slice_index[0],len(trigB))) \n",
    "trigB_long = pd.concat([trigB_20,trigB_21])\n",
    "\n",
    "trigC_20 = trigC.drop(np.arange(slice_index[1],len(trigC)))\n",
    "trigC_long = pd.concat([trigC_20,trigC_21])\n",
    "\n",
    "trigM_20 = trigM.drop(np.arange(slice_index[2],len(trigM)))\n",
    "trigM_long = pd.concat([trigM_20,trigM_21])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-howard",
   "metadata": {},
   "source": [
    "## IPN Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-costa",
   "metadata": {},
   "source": [
    "### IPN_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "racial-nirvana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1176,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Old IPN datetime list\n",
    "ipn_dt_temp = []\n",
    "ipn_dict = ipn_old_df.to_dict(\"records\")\n",
    "for row in ipn_dict:\n",
    "    datetime_str = row[\"datetime\"]\n",
    "    datetime_obj = datetime.strptime(datetime_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    ipn_dt_temp.append(datetime_obj)\n",
    "    \n",
    "ipn_old_dt = np.asarray(ipn_dt_temp)\n",
    "ipn_old_dt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-warning",
   "metadata": {},
   "source": [
    "### IPN new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "statutory-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-padding hour in times\n",
    "\n",
    "def zero_padding(l):\n",
    "    \"\"\"Function for zero padding times in IPN new\n",
    "    Parameters:\n",
    "    -----\n",
    "    l <array> containing string of times\n",
    "\n",
    "    Returns:\n",
    "    -----\n",
    "    new_times <array> containing string of zero-padded times\n",
    "    \"\"\"\n",
    "    new_times = []\n",
    "\n",
    "    for time in l:\n",
    "        if time.index(\":\") < 2:\n",
    "            time = \"0\" + time\n",
    "            new_times.append(time)\n",
    "        else:\n",
    "            new_times.append(time)\n",
    "    return np.array(new_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "authentic-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorization of names and times from dataframe.\n",
    "names = ipn_new_df[\"GRB_name\"].values\n",
    "times = ipn_new_df[\"T0(UTC)\"].values  # Hour is not zero padded.\n",
    "new_times = zero_padding(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "least-annex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining datetime objects in ipn_new_list.\n",
    "# Problem: some of the times have microsecond resolution while other dont\n",
    "ipn_new_list = []\n",
    "if len(names) == len(new_times):\n",
    "    for i in np.arange(0, len(names)):\n",
    "        date = names[i][3:9]\n",
    "        time = new_times[i]\n",
    "\n",
    "        # Managing the microsecond resolution\n",
    "        if len(time) > 8:\n",
    "            dt_obj = datetime.strptime(date + \" \" + time, \"%y%m%d %H:%M:%S.%f\")\n",
    "        else:\n",
    "            dt_obj = datetime.strptime(date + \" \" + time, \"%y%m%d %H:%M:%S\")\n",
    "            \n",
    "        ipn_new_list.append(dt_obj)\n",
    "else:\n",
    "    print(\"Unequal length of arrays\")\n",
    "    \n",
    "#Earliest entry in the ASIM data 2018,6,1,12,46,8\n",
    "# Including only entries up to 1142. That means entries in IPN starts from 2018, 5, 29, 8, 29, 14\n",
    "\n",
    "ipn_new_dt = np.asarray(ipn_new_list[:1142][::-1]) #Slicing the list. Returning in reverse order (decending)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-protein",
   "metadata": {
    "code_folding": [
     1,
     8,
     17
    ]
   },
   "outputs": [],
   "source": [
    "#Function for stripping the microseconds from an datetime object. Not necessary\n",
    "def strip_microseconds(l):\n",
    "    \"\"\"\n",
    "    Strip microseconds from datetime objects in list containing a tuple of dt objects and int. Returning string\n",
    "    objects. Seperating them in two arrays\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    l : list of datetime objects\n",
    "        The list containing the objects.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    updated_list : array of stripped datetime objects as string and array of trig_id\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    TypeError\n",
    "        If the input is not a type list or array\n",
    "    \"\"\"\n",
    "    time = []\n",
    "    trig_id = []\n",
    "    try:\n",
    "        for tup in l:\n",
    "            dt = tup[0].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            time.append(dt)\n",
    "            trig_id.append(tup[1])\n",
    "            \n",
    "        return np.array((time,trig_id))\n",
    "    \n",
    "    except TypeError:\n",
    "        print(\"Input is not the correct type <List> or <nd array> or entries is not tuple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Important to keep the microseconds? Check the search algo\n",
    "ipn_old_updated = strip_microseconds(matches_old)\n",
    "ipn_new_updated = strip_microseconds(matches_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-bracelet",
   "metadata": {},
   "source": [
    "## ASIM datetime correction, vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "turkish-radical",
   "metadata": {
    "code_folding": [
     14,
     27
    ]
   },
   "outputs": [],
   "source": [
    "# Function for correcting date and time in ASIM data. Returning ndarray that contains datetime objects\n",
    "\n",
    "def corr_dt(dfs):  # Correcting times from ASIM data and returning nested array of datetime objects\n",
    "    \n",
    "    \"\"\"   This function corrects the time from ASIM data and returns datetimeobjects\n",
    "    The correction is done by subtracting the correction from the original time.\n",
    "    The correction is given as a string.\n",
    "    The function takes a list of dataframes as input.\n",
    "    The function returns a list of arrays containing the corrected datetime objects.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    MAYBE LISTS CONTAINING STRING OF tuple(DATE,TIME)?\n",
    "    \n",
    "    dfs : list of dataframes\n",
    "        The dataframes containing the data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    trig_dt : list of arrays\n",
    "        The corrected datetime objects.\n",
    "        trig_dt[0] --> trigB\n",
    "        trig_dt[1] --> trigC\n",
    "        trig_dt[2] --> trigM\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the lists are not the same length.\n",
    "    \"\"\"\n",
    "    trig_dt = []\n",
    "    \n",
    "    for df in dfs:\n",
    "        temp_dt = []\n",
    "        # Vectorization of columns\n",
    "        date = df[\"date\"].values  # date given as string.\n",
    "        time = df[\"time\"].values  # time given as string\n",
    "        corr = df[\"corr\"].values  # correction given as string\n",
    "        try:\n",
    "            if len(date) and len(time) != len(corr):\n",
    "                raise ValueError\n",
    "        except:\n",
    "            raise ValueError(\"Lists are not the same length\")\n",
    "        else:\n",
    "            for i in np.arange(0, len(corr)):  # Iterating over the vectors\n",
    "                if corr[i] == \"--------\":  # No correction needed. Appending the datetime object\n",
    "                    date_str = date[i]\n",
    "                    time_str = time[i]\n",
    "                    org_dt = datetime.strptime(\n",
    "                        date_str + \" \" + time_str, \"%Y-%b-%d %H:%M:%S.%f\")\n",
    "                    temp_dt.append(org_dt)\n",
    "                    \n",
    "                elif corr[i][0] == \"-\":  # If it's a \"-\" in front; correction is added\n",
    "                    # formatting the datetime object\n",
    "                    date_str = date[i]\n",
    "                    time_str = time[i]\n",
    "                    org_dt = datetime.strptime(\n",
    "                        date_str + \" \" + time_str, \"%Y-%b-%d %H:%M:%S.%f\")  # Original datetime\n",
    "\n",
    "                    micro_corr = int(corr[0][1:])\n",
    "\n",
    "                    # new corrected datetime. Timedelta ccounts for changes in seconds also\n",
    "                    new_dt = org_dt + timedelta(microseconds=micro_corr)\n",
    "                    temp_dt.append(new_dt)\n",
    "                else:\n",
    "                    date_str = date[i]\n",
    "                    time_str = time[i]\n",
    "                    org_dt = datetime.strptime(\n",
    "                        date_str + \" \" + time_str, \"%Y-%b-%d %H:%M:%S.%f\")  # Original datetime\n",
    "\n",
    "                    micro_corr = int(corr[0][1:])\n",
    "\n",
    "                    # new corrected datetime. Timedelta ccounts for changes in seconds also\n",
    "                    new_dt = org_dt - timedelta(microseconds=micro_corr)\n",
    "                    temp_dt.append(new_dt)\n",
    "                    \n",
    "            trig_dt.append(temp_dt)\n",
    "\n",
    "    trig_dt = np.array(trig_dt, dtype=object)\n",
    "    return trig_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cardiac-straight",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Callig corr_dt with a list containing the dataframes from ASIM\n",
    "trig_master = corr_dt([trigB_long,trigC_long,trigM_long])\n",
    "\n",
    "#Creating array instances in trig_dt\n",
    "for i in np.arange(0,len(trig_master)):\n",
    "    trig_master[i] = np.asarray(trig_master[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-contents",
   "metadata": {},
   "source": [
    "## ASIM and IPN search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "prime-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most compact algorithm for searching: np.where()\n",
    "#Storing matches in the match list. Stored as a tuple containing (datetime IPN, index trig_B)\n",
    "\n",
    "# TODO: wrap a function around it so it can take in several triggers (trigB,trigC etc..)\n",
    "def find_match(trig_arr):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ------\n",
    "    trig_arr <array> <datetime>\n",
    "    np.where returns: An array with elements from x where condition is True, and elements from y elsewhere.\n",
    "    \n",
    "    elements in matches_* contains tuples for indecies in IPN list and trig_arr. \"\"\"\n",
    "    matches_old = [] \n",
    "\n",
    "    for i in ipn_old_dt:\n",
    "        #Searching for matches that are +/- 10 seconds from the IPN trigger\n",
    "        mask = np.where((i-timedelta(seconds=10) <= trig_arr) & (trig_arr <= i + timedelta(seconds=10)))\n",
    "        if len(mask[0]) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            matches_old.append((i,mask[0]))\n",
    "\n",
    "    matches_new = []\n",
    "    for i in ipn_new_dt:\n",
    "        #Searching for matches that are +/- 10 seconds from the IPN trigger\n",
    "        mask = np.where((i-timedelta(seconds=10) <= trig_arr) & (trig_arr <= i + timedelta(seconds=10)))\n",
    "        #print(mask)\n",
    "        if len(mask[0]) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            matches_new.append((i,mask[0]))\n",
    "        \n",
    "    return (matches_old, matches_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "confused-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_match = []\n",
    "for i in trig_master:\n",
    "    tup_match = find_match(i)\n",
    "    master_match.append(tup_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "broken-translator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(master_match[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-deposit",
   "metadata": {},
   "source": [
    "## Branching\n",
    "Branching into matches in old ipn dataset and matches in new ipn dataset.\n",
    "\n",
    "Export into a dataframe containing IPN dt trigger | trig* dt trigger\n",
    "\n",
    "\n",
    "\n",
    "How to access data from master_match:\n",
    "\n",
    "\n",
    "- master_match[n] - refers to type of trigger (trigB, trigC, trigM)\n",
    "\n",
    "- master_match[n][n] - refers to old or new ipn trigger list [0]: old , [1]: new\n",
    "\n",
    "- master_match[n][n][n] - refers to tuple containing (ipn dt, array[index]) where index is the dt in trig_dt[n][index]\n",
    "\n",
    "- master_match[n][n][n][n] - selecting desired element in tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-concern",
   "metadata": {},
   "source": [
    "### Branching into seperate arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "sensitive-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_trigB_old = np.array(master_match[0][0],dtype=object)\n",
    "match_trigB_new = np.array(master_match[0][1],dtype=object)\n",
    "\n",
    "match_trigC_old = np.array(master_match[1][0],dtype=object)\n",
    "match_trigC_new = np.array(master_match[1][1],dtype=object)\n",
    "\n",
    "match_trigM_old = np.array(master_match[2][0],dtype=object)\n",
    "match_trigM_new = np.array(master_match[2][1],dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-generator",
   "metadata": {},
   "source": [
    "### Finding datetime object from index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "artistic-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_dt(i,arr_index):\n",
    "    \"\"\"Retrives the datetime object specified.\n",
    "    Parameters:\n",
    "    -------\n",
    "    i <int> index in the trig_dt array. Specifying which trigger to access\n",
    "    arr_index <array><int> index of the datetime object in the array\n",
    "    \n",
    "    Returns:\n",
    "    ------\n",
    "    <datetime> the specified datetimeobject\"\"\"\n",
    "    dts = []\n",
    "    for index in arr_index:\n",
    "        dt = trig_master[i][index]\n",
    "        dts.append(dt)\n",
    "    return dts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-drink",
   "metadata": {},
   "source": [
    "## Creating new dataframes with matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "elder-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe for ipn old and ipn new. Columns should be ipn dt trigger, trigB, trigC,trigM\n",
    "# if there isnt a trigger in the respected trigger, the entry should be None\n",
    "# But first. Produce df of ipn --> trigB, trigC, trigM [indecies]. Just indecies because there are several in some triggers\n",
    "\n",
    "trigB_old_dict = {\"ipn old dt\":[], \"trigB index\": []}\n",
    "for i in match_trigB_old:\n",
    "    trigB_old_dict[\"ipn old dt\"].append(i[0])\n",
    "    trigB_old_dict[\"trigB index\"].append(i[1])\n",
    "trigB_old_df = pd.DataFrame(trigB_old_dict)\n",
    "\n",
    "trigB_new_dict = {\"ipn new dt\":[], \"trigB index\": []}\n",
    "for i in match_trigB_new:\n",
    "    trigB_new_dict[\"ipn new dt\"].append(i[0])\n",
    "    trigB_new_dict[\"trigB index\"].append(i[1])\n",
    "trigB_new_df = pd.DataFrame(trigB_new_dict)\n",
    "\n",
    "trigC_old_dict = {\"ipn old dt\":[], \"trigC index\": []}\n",
    "for i in match_trigC_old:\n",
    "    trigC_old_dict[\"ipn old dt\"].append(i[0])\n",
    "    trigC_old_dict[\"trigC index\"].append(i[1])\n",
    "trigC_old_df = pd.DataFrame(trigC_old_dict)\n",
    "\n",
    "trigC_new_dict = {\"ipn new dt\":[], \"trigC index\": []}\n",
    "for i in match_trigC_new:\n",
    "    trigC_new_dict[\"ipn new dt\"].append(i[0])\n",
    "    trigC_new_dict[\"trigC index\"].append(i[1])\n",
    "trigC_new_df = pd.DataFrame(trigC_old_dict)\n",
    "\n",
    "trigM_old_dict = {\"ipn old dt\":[], \"trigM index\": []}\n",
    "for i in match_trigM_old:\n",
    "    trigM_old_dict[\"ipn old dt\"].append(i[0])\n",
    "    trigM_old_dict[\"trigM index\"].append(i[1])\n",
    "trigM_old_df = pd.DataFrame(trigM_old_dict)\n",
    "\n",
    "trigM_new_dict = {\"ipn new dt\":[], \"trigM index\": []}\n",
    "for i in match_trigM_new:\n",
    "    trigM_new_dict[\"ipn new dt\"].append(i[0])\n",
    "    trigM_new_dict[\"trigM index\"].append(i[1])\n",
    "trigM_new_df = pd.DataFrame(trigM_new_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-nepal",
   "metadata": {},
   "source": [
    "### Appending datetime objects as string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-trouble",
   "metadata": {},
   "source": [
    "#### IPN df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "integral-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigB_old_indecies = trigB_old_df[\"trigB index\"].values\n",
    "trigB_old_dts = index_to_dt(0,trigB_old_indecies)\n",
    "\n",
    "trigC_old_indecies = trigC_old_df[\"trigC index\"].values\n",
    "trigC_old_dts = index_to_dt(1,trigC_old_indecies)\n",
    "\n",
    "trigM_old_indecies = trigM_old_df[\"trigM index\"].values\n",
    "trigM_old_dts = index_to_dt(2,trigM_old_indecies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "institutional-assets",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigB_old_df[\"trigB dt\"] = trigB_old_dts\n",
    "trigC_old_df[\"trigC dt\"] = trigC_old_dts\n",
    "trigM_old_df[\"trigM dt\"] = trigM_old_dts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-demonstration",
   "metadata": {},
   "source": [
    "####  ICECUBE df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "numerous-triangle",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigB_new_indecies = trigB_new_df[\"trigB index\"].values\n",
    "trigB_new_dts = index_to_dt(0,trigB_new_indecies)\n",
    "\n",
    "trigC_new_indecies = trigC_new_df[\"trigC index\"].values\n",
    "trigC_new_dts = index_to_dt(1,trigC_new_indecies)\n",
    "\n",
    "trigM_new_indecies = trigM_new_df[\"trigM index\"].values\n",
    "trigM_new_dts = index_to_dt(2,trigM_new_indecies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "composite-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigB_new_df[\"trigB dt\"] = trigB_new_dts\n",
    "trigC_new_df[\"trigC dt\"] = trigC_new_dts\n",
    "trigM_new_df[\"trigM dt\"] = trigM_new_dts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "inner-preference",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ipn old dt</th>\n",
       "      <th>trigC index</th>\n",
       "      <th>trigC dt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-27 06:16:59</td>\n",
       "      <td>[11202]</td>\n",
       "      <td>[2018-12-27 06:17:04.210500]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-17 08:50:43</td>\n",
       "      <td>[12923]</td>\n",
       "      <td>[2019-01-17 08:50:42.939689]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-04-20 23:32:24</td>\n",
       "      <td>[20754]</td>\n",
       "      <td>[2019-04-20 23:32:24.931211]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-01 05:23:21</td>\n",
       "      <td>[21437]</td>\n",
       "      <td>[2019-05-01 05:23:21.736797]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-06-15 14:42:25</td>\n",
       "      <td>[23861]</td>\n",
       "      <td>[2019-06-15 14:42:22.741579]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-06-28 04:23:39</td>\n",
       "      <td>[24588]</td>\n",
       "      <td>[2019-06-28 04:23:32.512290]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-08-13 12:00:45</td>\n",
       "      <td>[27017]</td>\n",
       "      <td>[2019-08-13 12:00:48.801260]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-08-25 21:04:56</td>\n",
       "      <td>[27707, 27708]</td>\n",
       "      <td>[2019-08-25 21:04:53.870393, 2019-08-25 21:04:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-09-06 01:04:53</td>\n",
       "      <td>[28416]</td>\n",
       "      <td>[2019-09-06 01:04:52.014004]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-10-01 06:41:50</td>\n",
       "      <td>[30103]</td>\n",
       "      <td>[2019-10-01 06:41:51.631214]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-10-04 18:07:02</td>\n",
       "      <td>[30481]</td>\n",
       "      <td>[2019-10-04 18:07:03.402478]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-11-19 10:41:09</td>\n",
       "      <td>[36246]</td>\n",
       "      <td>[2019-11-19 10:41:05.239944]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019-12-21 20:39:13</td>\n",
       "      <td>[37531, 37532, 37533, 37534, 37535]</td>\n",
       "      <td>[2019-12-21 20:39:11.939846, 2019-12-21 20:39:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019-12-27 17:21:44</td>\n",
       "      <td>[38107]</td>\n",
       "      <td>[2019-12-27 17:21:49.396926]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2020-01-11 15:11:08</td>\n",
       "      <td>[40031]</td>\n",
       "      <td>[2020-01-11 15:11:13.187473]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2020-01-22 05:18:20</td>\n",
       "      <td>[41477]</td>\n",
       "      <td>[2020-01-22 05:18:10.228382]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2020-02-24 09:58:44</td>\n",
       "      <td>[45445]</td>\n",
       "      <td>[2020-02-24 09:58:44.448334]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2020-04-15 08:48:05</td>\n",
       "      <td>[53009]</td>\n",
       "      <td>[2020-04-15 08:48:05.470385]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2020-04-23 13:54:11</td>\n",
       "      <td>[54213]</td>\n",
       "      <td>[2020-04-23 13:54:06.572991]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2020-05-25 14:40:28</td>\n",
       "      <td>[57169]</td>\n",
       "      <td>[2020-05-25 14:40:23.102170]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2020-06-05 18:17:42</td>\n",
       "      <td>[58763]</td>\n",
       "      <td>[2020-06-05 18:17:42.274631]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2020-08-12 11:55:28</td>\n",
       "      <td>[66374, 66375]</td>\n",
       "      <td>[2020-08-12 11:55:24.122194, 2020-08-12 11:55:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2020-09-03 21:38:58</td>\n",
       "      <td>[69085]</td>\n",
       "      <td>[2020-09-03 21:38:50.495317]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2020-09-07 18:51:11</td>\n",
       "      <td>[69619]</td>\n",
       "      <td>[2020-09-07 18:51:20.724118]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2020-09-15 03:27:15</td>\n",
       "      <td>[70629]</td>\n",
       "      <td>[2020-09-15 03:27:06.777086]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2020-09-23 17:57:43</td>\n",
       "      <td>[71775]</td>\n",
       "      <td>[2020-09-23 17:57:42.645020]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2020-11-04 00:00:56</td>\n",
       "      <td>[75674]</td>\n",
       "      <td>[2020-11-04 00:00:56.665807]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2020-11-09 02:31:05</td>\n",
       "      <td>[76066]</td>\n",
       "      <td>[2020-11-09 02:31:09.400868]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2021-01-02 20:38:11</td>\n",
       "      <td>[79197, 79198]</td>\n",
       "      <td>[2021-01-02 20:38:02.245585, 2021-01-02 20:38:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2021-04-24 08:01:56</td>\n",
       "      <td>[89577]</td>\n",
       "      <td>[2021-04-24 08:01:55.875520]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2021-06-07 21:39:20</td>\n",
       "      <td>[92671, 92672, 92673]</td>\n",
       "      <td>[2021-06-07 21:39:21.187641, 2021-06-07 21:39:...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ipn old dt                          trigC index  \\\n",
       "0  2018-12-27 06:16:59                              [11202]   \n",
       "1  2019-01-17 08:50:43                              [12923]   \n",
       "2  2019-04-20 23:32:24                              [20754]   \n",
       "3  2019-05-01 05:23:21                              [21437]   \n",
       "4  2019-06-15 14:42:25                              [23861]   \n",
       "5  2019-06-28 04:23:39                              [24588]   \n",
       "6  2019-08-13 12:00:45                              [27017]   \n",
       "7  2019-08-25 21:04:56                       [27707, 27708]   \n",
       "8  2019-09-06 01:04:53                              [28416]   \n",
       "9  2019-10-01 06:41:50                              [30103]   \n",
       "10 2019-10-04 18:07:02                              [30481]   \n",
       "11 2019-11-19 10:41:09                              [36246]   \n",
       "12 2019-12-21 20:39:13  [37531, 37532, 37533, 37534, 37535]   \n",
       "13 2019-12-27 17:21:44                              [38107]   \n",
       "14 2020-01-11 15:11:08                              [40031]   \n",
       "15 2020-01-22 05:18:20                              [41477]   \n",
       "16 2020-02-24 09:58:44                              [45445]   \n",
       "17 2020-04-15 08:48:05                              [53009]   \n",
       "18 2020-04-23 13:54:11                              [54213]   \n",
       "19 2020-05-25 14:40:28                              [57169]   \n",
       "20 2020-06-05 18:17:42                              [58763]   \n",
       "21 2020-08-12 11:55:28                       [66374, 66375]   \n",
       "22 2020-09-03 21:38:58                              [69085]   \n",
       "23 2020-09-07 18:51:11                              [69619]   \n",
       "24 2020-09-15 03:27:15                              [70629]   \n",
       "25 2020-09-23 17:57:43                              [71775]   \n",
       "26 2020-11-04 00:00:56                              [75674]   \n",
       "27 2020-11-09 02:31:05                              [76066]   \n",
       "28 2021-01-02 20:38:11                       [79197, 79198]   \n",
       "29 2021-04-24 08:01:56                              [89577]   \n",
       "30 2021-06-07 21:39:20                [92671, 92672, 92673]   \n",
       "\n",
       "                                             trigC dt  \n",
       "0                        [2018-12-27 06:17:04.210500]  \n",
       "1                        [2019-01-17 08:50:42.939689]  \n",
       "2                        [2019-04-20 23:32:24.931211]  \n",
       "3                        [2019-05-01 05:23:21.736797]  \n",
       "4                        [2019-06-15 14:42:22.741579]  \n",
       "5                        [2019-06-28 04:23:32.512290]  \n",
       "6                        [2019-08-13 12:00:48.801260]  \n",
       "7   [2019-08-25 21:04:53.870393, 2019-08-25 21:04:...  \n",
       "8                        [2019-09-06 01:04:52.014004]  \n",
       "9                        [2019-10-01 06:41:51.631214]  \n",
       "10                       [2019-10-04 18:07:03.402478]  \n",
       "11                       [2019-11-19 10:41:05.239944]  \n",
       "12  [2019-12-21 20:39:11.939846, 2019-12-21 20:39:...  \n",
       "13                       [2019-12-27 17:21:49.396926]  \n",
       "14                       [2020-01-11 15:11:13.187473]  \n",
       "15                       [2020-01-22 05:18:10.228382]  \n",
       "16                       [2020-02-24 09:58:44.448334]  \n",
       "17                       [2020-04-15 08:48:05.470385]  \n",
       "18                       [2020-04-23 13:54:06.572991]  \n",
       "19                       [2020-05-25 14:40:23.102170]  \n",
       "20                       [2020-06-05 18:17:42.274631]  \n",
       "21  [2020-08-12 11:55:24.122194, 2020-08-12 11:55:...  \n",
       "22                       [2020-09-03 21:38:50.495317]  \n",
       "23                       [2020-09-07 18:51:20.724118]  \n",
       "24                       [2020-09-15 03:27:06.777086]  \n",
       "25                       [2020-09-23 17:57:42.645020]  \n",
       "26                       [2020-11-04 00:00:56.665807]  \n",
       "27                       [2020-11-09 02:31:09.400868]  \n",
       "28  [2021-01-02 20:38:02.245585, 2021-01-02 20:38:...  \n",
       "29                       [2021-04-24 08:01:55.875520]  \n",
       "30  [2021-06-07 21:39:21.187641, 2021-06-07 21:39:...  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigC_new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-cuisine",
   "metadata": {},
   "source": [
    "## Exporting matches from IPN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-revolution",
   "metadata": {},
   "source": [
    "### trigB export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-sustainability",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in trigB_old_df.iterrows():\n",
    "    trigB_old_df[\"trigB dt\"].iloc[index] = []\n",
    "    for i in row[2]:\n",
    "        trigB_old_df[\"trigB dt\"].iloc[index].append(datetime.strftime(i,\"%Y-%m-%d %H:%M:%S.%f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigB_old_df.to_csv(\"{}/trigB_old_match.csv\".format(IPN_match_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-runner",
   "metadata": {},
   "source": [
    "### trigC export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "economic-punch",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "for index, row in trigC_old_df.iterrows():\n",
    "    trigC_old_df[\"trigC dt\"].iloc[index] = []\n",
    "    for i in row[2]:\n",
    "        trigC_old_df[\"trigC dt\"].iloc[index].append(datetime.strftime(i,\"%Y-%m-%d %H:%M:%S.%f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "italic-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigC_old_df.to_csv(\"{}/trigC_old_match.csv\".format(IPN_match_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promotional-material",
   "metadata": {},
   "source": [
    "### trigM export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in trigM_old_df.iterrows():\n",
    "    trigM_old_df[\"trigM dt\"].iloc[index] = []\n",
    "    for i in row[2]:\n",
    "        trigM_old_df[\"trigM dt\"].iloc[index].append(datetime.strftime(i,\"%Y-%m-%d %H:%M:%S.%f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-conducting",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigM_old_df.to_csv(\"trigM_old_match.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-funds",
   "metadata": {},
   "source": [
    "### trigger lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigB_strf = []\n",
    "for dt in trig_dt[0]:\n",
    "    strf = dt.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    trigB_strf.append(strf)\n",
    "    \n",
    "    \n",
    "trigB_df_exp = pd.DataFrame({\"datetime\": trigB_strf})\n",
    "trigB_df_exp.to_csv(\"HED triggers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-flash",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigC_strf = []\n",
    "for dt in trig_dt[1]:\n",
    "    strf = dt.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    trigC_strf.append(strf)\n",
    "    \n",
    "    \n",
    "trigC_df_exp = pd.DataFrame({\"datetime\": trigC_strf})\n",
    "trigC_df_exp.to_csv(\"LED triggers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigM_strf = []\n",
    "for dt in trig_dt[2]:\n",
    "    strf = dt.strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "    trigM_strf.append(strf)\n",
    "    \n",
    "    \n",
    "trigM_df_exp = pd.DataFrame({\"datetime\": trigM_strf})\n",
    "trigM_df_exp.to_csv(\"MMIA triggers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-acrobat",
   "metadata": {},
   "source": [
    "## ICECUBE export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "european-reservation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "#Formatting datetimeobjects to strings\n",
    "for index, row in trigB_new_df.iterrows():\n",
    "    trigB_new_df[\"trigB dt\"].iloc[index] = []\n",
    "    for i in row[2]:\n",
    "        trigB_new_df[\"trigB dt\"].iloc[index].append(datetime.strftime(i,\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "        \n",
    "trigB_new_df.to_csv(\"{}/HED_new_match.csv\".format(ICECUBE_match_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "immune-luxury",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatting datetimeobjects to strings\n",
    "for index, row in trigC_new_df.iterrows():\n",
    "    trigC_new_df[\"trigC dt\"].iloc[index] = []\n",
    "    for i in row[2]:\n",
    "        trigC_new_df[\"trigC dt\"].iloc[index].append(datetime.strftime(i,\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "        \n",
    "trigC_new_df.to_csv(\"{}/LED_new_match.csv\".format(ICECUBE_match_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "original-spread",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formatting datetimeobjects to strings\n",
    "for index, row in trigM_new_df.iterrows():\n",
    "    trigM_new_df[\"trigM dt\"].iloc[index] = []\n",
    "    for i in row[2]:\n",
    "        trigM_new_df[\"trigM dt\"].iloc[index].append(datetime.strftime(i,\"%Y-%m-%d %H:%M:%S.%f\"))\n",
    "        \n",
    "trigM_new_df.to_csv(\"{}/MMIA_new_match.csv\".format(ICECUBE_match_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-transfer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-cookie",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-snake",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-coalition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-terminal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "thousand-addiction",
   "metadata": {},
   "source": [
    "### Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-catholic",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function to filter duplicates\n",
    "def filter_duplicates(l1,l2):\n",
    "    \n",
    "    l1 --> ipn_old match\n",
    "    l2 --> ipn_new match\n",
    "    \n",
    "    Converting l1,l2 to np.arrays to handle the filtering. (might not be neccecery)\n",
    "    Checking if dt object in l2 is not present in l1. If so, then add the tuple to l1\n",
    "    Parameters: l1,l2 <list> containing tuples\n",
    "    ----\n",
    "    Returns: l3 <array> of dt objects that are not in \n",
    "    \n",
    "    found_time = []\n",
    "    indecies = []\n",
    "    for index, i in enumerate(ipn_new_updated[0]):\n",
    "        if i not in ipn_old_updated:\n",
    "            found_time.append(i)\n",
    "            indecies.append(index)\n",
    "        \n",
    "    found_time = np.asarray(found_time)\n",
    "    indecies = np.asarray(indecies) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-loading",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
