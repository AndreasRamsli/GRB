{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "honey-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-plastic",
   "metadata": {},
   "source": [
    "## Objective:\n",
    "-Import the datasets. Still need the extra dataset. Includes triggers up to dec. 21\n",
    "\n",
    "-datetime helpful docs: https://www.w3schools.com/python/python_datetime.asp\n",
    "\n",
    "-Create algorithm that searches for matches that are +/- 10 second apart and remove the rest (maybe two seconds)\n",
    "\n",
    "-First match found at 2018, 7, 11, 17, 2, 4\n",
    "\n",
    "-Complete match list found for trigB --> trigB_match\n",
    "\n",
    "//TODO: \n",
    "\n",
    "- Update list up until 2022\n",
    "- Wrap functions around the algorithms so that the same can be done for trigC etc.\n",
    "- Create dataframe with the result\n",
    "-Access the relevant files found in trigB_match list\n",
    "-Do spectral analysis on the data (counts/bin)\n",
    "\n",
    "\n",
    "### Notes from stackoverflow:\n",
    "np.where() implementation: https://stackoverflow.com/questions/35714902/find-location-of-pair-of-elements-in-two-arrays-in-numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-steal",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "lucky-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing and redefining the dataframes\n",
    "#Not loading the trigger list for 2021 just yet\n",
    "\n",
    "#Old IPN dataset\n",
    "ipn_old_data = pd.read_csv(\"./IPN/trigIPN.csv\", sep=\"|\")\n",
    "ipn_old_df = pd.DataFrame(ipn_old_data)\n",
    "ipn_old_df.drop(columns= ['Unnamed: 0', 'Unnamed: 2'], axis=1, inplace=True) #dropping unwanted columns\n",
    "ipn_old_df.rename(columns={ipn_old_df.columns[0]:\"datetime\"}, inplace = True)\n",
    "\n",
    "#New IPN dataset\n",
    "ipn_new_data = pd.read_csv(\"./IPN/ipn_supp.txt\",sep = \"\\s+|\\t+|\\s+\\t+|\\t+\\s+\", engine=\"python\")\n",
    "ipn_new_df = pd.DataFrame(ipn_new_data)\n",
    "#Dropping unwanted columns\n",
    "ipn_new_df = ipn_new_df.drop([\"GRB_name_Fermi\",'ra(°J2000)', 'decl(°J2000)',\n",
    "       'pos_error(°1-sigma)', 'T90(s)', 'T90_error(s)', 'T90_start(UTC)',\n",
    "       'fluence(erg/cm^2)', 'fluence_error(erg/cm^2)', 'redshift', 'T100(s)',\n",
    "       'GBM_located', 'mjd(T0day)'], 1)\n",
    "\n",
    "#HED \n",
    "trigB_data = pd.read_csv(\"./ASIM/trigB.txt\", sep = \"\\s+|\\t+|\\s+\\t+|\\t+\\s+\", engine=\"python\")\n",
    "trigB = pd.DataFrame(trigB_data)\n",
    "trigB.drop(columns=[\"######\"], inplace=True) #dropping unwanted columns\n",
    "trigB.rename(columns={\"yyyy-MMM-dd\":\"date\",\"HH:mm:ss.SSSSSS\": \"time\", \"Corr\":\"corr\"}, inplace=True) #renaming columns\n",
    "\n",
    "trigB_21_data = pd.read_csv(\"./ASIM/trigB_2021.txt\", sep = \"\\s+|\\t+|\\s+\\t+|\\t+\\s+\", engine=\"python\")\n",
    "trigB_21 = pd.DataFrame(trigB_21_data)\n",
    "trigB_21.drop(columns=[\"######\"], inplace=True)\n",
    "trigB_21.rename(columns={\"yyyy-MMM-dd\":\"date\",\"HH:mm:ss.SSSSSS\": \"time\" ,\"Corr\":\"corr\"}, inplace=True)\n",
    "\n",
    "#LED\n",
    "trigC_data = pd.read_csv(\"./ASIM/trigC.txt\", sep = \"\\s+|\\t+|\\s+\\t+|\\t+\\s+\", engine=\"python\")\n",
    "trigC = pd.DataFrame(trigC_data)\n",
    "trigC.drop(columns=[\"######\"], inplace=True)\n",
    "trigC.rename(columns={\"yyyy-MMM-dd\":\"date\",\"HH:mm:ss.SSSSSS\": \"time\" ,\"Corr\":\"corr\"}, inplace=True)\n",
    "\n",
    "trigC_21_data = pd.read_csv(\"./ASIM/trigC_2021.txt\", sep = \"\\s+|\\t+|\\s+\\t+|\\t+\\s+\", engine=\"python\")\n",
    "trigC_21 = pd.DataFrame(trigC_21_data)\n",
    "trigC_21.drop(columns=[\"######\"], inplace=True)\n",
    "trigC_21.rename(columns={\"yyyy-MMM-dd\":\"date\",\"HH:mm:ss.SSSSSS\": \"time\", 'Corr':\"corr\" }, inplace=True)\n",
    "\n",
    "#MMIA (visible)\n",
    "trigM_data = pd.read_csv(\"./ASIM/trigM.txt\", sep = \"\\s+|\\t+|\\s+\\t+|\\t+\\s+\", engine=\"python\")\n",
    "trigM = pd.DataFrame(trigM_data)\n",
    "trigM.drop(columns=[\"######\"], inplace=True)\n",
    "trigM.rename(columns={\"yyyy-MMM-dd\":\"date\",\"HH:mm:ss.SSSSSS\": \"time\", \"Corr\":\"corr\" }, inplace=True)\n",
    "\n",
    "trigM_21_data = pd.read_csv(\"./ASIM/trigM_2021.txt\", sep = \"\\s+|\\t+|\\s+\\t+|\\t+\\s+\", engine=\"python\")\n",
    "trigM_21 = pd.DataFrame(trigM_21_data)\n",
    "trigM_21.drop(columns=[\"######\"], inplace=True)\n",
    "trigM_21.rename(columns={\"yyyy-MMM-dd\":\"date\",\"HH:mm:ss.SSSSSS\": \"time\", \"Corr\":\"corr\" }, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-sector",
   "metadata": {},
   "source": [
    "## Concatenating ASIM dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "disabled-lounge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_21(l):\n",
    "    \"\"\"\n",
    "    Function for finding the indecies to slice. Assuming l is sorted by date from first to last.\n",
    "    Finding index of first entry of '2021-Jan-01'.\n",
    "    \n",
    "    Parameters:\n",
    "    -----\n",
    "    l <array>\n",
    "    \n",
    "    Returns:\n",
    "    -----\n",
    "    Returning index for the first entry of '2021-Jan-01' \"\"\"\n",
    "    \n",
    "    \n",
    "    i = l.tolist().index('2021-Jan-01')\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "experimental-enough",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[110410, 79065, 237671]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice_index = []\n",
    "for df in [trigB,trigC,trigM]:\n",
    "    temp_date_list = df[\"date\"].values\n",
    "    slice_index.append(find_index_21(temp_date_list))\n",
    "slice_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "induced-collector",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trig*_20 contains rows to the end of 2020\n",
    "#trig*_master contains rows to the end of 2021\n",
    "trigB_20 = trigB.drop(np.arange(slice_index[0],len(trigB))) \n",
    "trigB_master = pd.concat([trigB_20,trigB_21])\n",
    "\n",
    "trigC_20 = trigC.drop(np.arange(slice_index[1],len(trigC)))\n",
    "trigC_master = pd.concat([trigC_20,trigC_21])\n",
    "\n",
    "trigM_20 = trigM.drop(np.arange(slice_index[2],len(trigM)))\n",
    "trigM_master = pd.concat([trigM_20,trigM_21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "consolidated-hollywood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(              date             time      corr\n",
       " 0      2018-Jun-01  12:46:08.843153   -191662\n",
       " 1      2018-Jun-01  12:51:01.726322   -192770\n",
       " 2      2018-Jun-01  13:06:13.113589   -196203\n",
       " 3      2018-Jun-01  13:12:04.912824   -197526\n",
       " 4      2018-Jun-01  13:14:19.594102   -198029\n",
       " ...            ...              ...       ...\n",
       " 37716  2021-Dec-31  23:05:01.249646    100055\n",
       " 37717  2021-Dec-31  23:33:47.967067  --------\n",
       " 37718  2021-Dec-31  23:36:28.826496  --------\n",
       " 37719  2021-Dec-31  23:38:39.674400  --------\n",
       " 37720  2021-Dec-31  23:42:49.746266  --------\n",
       " \n",
       " [148131 rows x 3 columns],\n",
       "               date             time      corr\n",
       " 0      2018-Jun-01  05:10:05.271627    -88533\n",
       " 1      2018-Jun-01  05:12:03.803744    -88987\n",
       " 2      2018-Jun-01  05:15:54.113081    -89864\n",
       " 3      2018-Jun-01  05:16:36.442279    -90024\n",
       " 4      2018-Jun-01  09:54:36.077906   -153002\n",
       " ...            ...              ...       ...\n",
       " 32628  2021-Dec-31  23:24:30.527051  --------\n",
       " 32629  2021-Dec-31  23:27:58.585979  --------\n",
       " 32630  2021-Dec-31  23:36:12.691052  --------\n",
       " 32631  2021-Dec-31  23:38:16.325004  --------\n",
       " 32632  2021-Dec-31  23:40:53.722474  --------\n",
       " \n",
       " [111698 rows x 3 columns],\n",
       "               date             time      corr\n",
       " 0      2018-Jun-06  14:45:52.545897    487063\n",
       " 1      2018-Jun-06  14:48:15.630088    487945\n",
       " 2      2018-Jun-06  14:48:22.379083    487982\n",
       " 3      2018-Jun-06  14:48:23.961985    487994\n",
       " 4      2018-Jun-06  14:48:29.045275    488025\n",
       " ...            ...              ...       ...\n",
       " 74188  2021-Dec-31  21:55:28.826504     74944\n",
       " 74189  2021-Dec-31  23:28:50.333508  --------\n",
       " 74190  2021-Dec-31  23:28:52.917332  --------\n",
       " 74191  2021-Dec-31  23:29:16.335874  --------\n",
       " 74192  2021-Dec-31  23:44:09.182887  --------\n",
       " \n",
       " [311864 rows x 3 columns])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigB_master,trigC_master,trigM_master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-hobby",
   "metadata": {},
   "source": [
    "## IPN Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-buying",
   "metadata": {},
   "source": [
    "### IPN_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "warming-audio",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1176,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Old IPN datetime list\n",
    "ipn_dt_temp = []\n",
    "ipn_dict = ipn_old_df.to_dict(\"records\")\n",
    "for row in ipn_dict:\n",
    "    datetime_str = row[\"datetime\"]\n",
    "    datetime_obj = datetime.strptime(datetime_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    ipn_dt_temp.append(datetime_obj)\n",
    "    \n",
    "ipn_old_dt = np.asarray(ipn_dt_temp)\n",
    "ipn_old_dt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-rough",
   "metadata": {},
   "source": [
    "### IPN new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "anticipated-prior",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-padding hour in times\n",
    "\n",
    "def zero_padding(l):\n",
    "    \"\"\"Function for zero padding times in IPN new\n",
    "    Parameters:\n",
    "    -----\n",
    "    l <array> containing string of times\n",
    "\n",
    "    Returns:\n",
    "    -----\n",
    "    new_times <array> containing string of zero-padded times\n",
    "    \"\"\"\n",
    "    new_times = []\n",
    "\n",
    "    for time in l:\n",
    "        if time.index(\":\") < 2:\n",
    "            time = \"0\" + time\n",
    "            new_times.append(time)\n",
    "        else:\n",
    "            new_times.append(time)\n",
    "    return np.array(new_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "hydraulic-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorization of names and times from dataframe.\n",
    "names = ipn_new_df[\"GRB_name\"].values\n",
    "times = ipn_new_df[\"T0(UTC)\"].values  # Hour is not zero padded.\n",
    "new_times = zero_padding(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "least-annex",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1142,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining datetime objects in ipn_new_list.\n",
    "# Problem: some of the times have microsecond resolution while other dont\n",
    "ipn_new_list = []\n",
    "if len(names) == len(new_times):\n",
    "    for i in np.arange(0, len(names)):\n",
    "        date = names[i][3:9]\n",
    "        time = new_times[i]\n",
    "\n",
    "        # Managing the microsecond resolution\n",
    "        if len(time) > 8:\n",
    "            dt_obj = datetime.strptime(date + \" \" + time, \"%y%m%d %H:%M:%S.%f\")\n",
    "        else:\n",
    "            dt_obj = datetime.strptime(date + \" \" + time, \"%y%m%d %H:%M:%S\")\n",
    "            \n",
    "        ipn_new_list.append(dt_obj)\n",
    "else:\n",
    "    print(\"Unequal length of arrays\")\n",
    "    \n",
    "#Earliest entry in the ASIM data 2018,6,1,12,46,8\n",
    "# Including only entries up to 1142. That means entries in IPN starts from 2018, 5, 29, 8, 29, 14\n",
    "\n",
    "ipn_new_dt = np.asarray(ipn_new_list[:1142][::-1]) #Slicing the list. Returning in reverse order (decending)\n",
    "ipn_new_dt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-opposition",
   "metadata": {
    "code_folding": [
     1,
     8,
     17
    ]
   },
   "outputs": [],
   "source": [
    "#Function for stripping the microseconds from an datetime object\n",
    "def strip_microseconds(l):\n",
    "    \"\"\"\n",
    "    Strip microseconds from datetime objects in list containing a tuple of dt objects and int. Returning string\n",
    "    objects. Seperating them in two arrays\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    l : list of datetime objects\n",
    "        The list containing the objects.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    updated_list : array of stripped datetime objects as string and array of trig_id\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    TypeError\n",
    "        If the input is not a type list or array\n",
    "    \"\"\"\n",
    "    time = []\n",
    "    trig_id = []\n",
    "    try:\n",
    "        for tup in l:\n",
    "            dt = tup[0].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            time.append(dt)\n",
    "            trig_id.append(tup[1])\n",
    "            \n",
    "        return np.array((time,trig_id))\n",
    "    \n",
    "    except TypeError:\n",
    "        print(\"Input is not the correct type <List> or <nd array> or entries is not tuple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-coaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Important to keep the microseconds? Check the search algo\n",
    "ipn_old_updated = strip_microseconds(matches_old)\n",
    "ipn_new_updated = strip_microseconds(matches_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-asset",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "invalid-bracelet",
   "metadata": {},
   "source": [
    "## ASIM datetime correction, vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "turkish-radical",
   "metadata": {
    "code_folding": [
     14,
     27
    ]
   },
   "outputs": [],
   "source": [
    "# Function for correcting date and time in ASIM data. Returning ndarray that contains datetime objects\n",
    "\n",
    "def corr_dt(dfs):  # Correcting times from ASIM data and returning nested array of datetime objects\n",
    "    \n",
    "    \"\"\"   This function corrects the time from ASIM data and returns datetimeobjects\n",
    "    The correction is done by subtracting the correction from the original time.\n",
    "    The correction is given as a string.\n",
    "    The function takes a list of dataframes as input.\n",
    "    The function returns a list of arrays containing the corrected datetime objects.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    MAYBE LISTS CONTAINING STRING OF tuple(DATE,TIME)?\n",
    "    \n",
    "    dfs : list of dataframes\n",
    "        The dataframes containing the data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    trig_dt : list of arrays\n",
    "        The corrected datetime objects.\n",
    "        trig_dt[0] --> trigB\n",
    "        trig_dt[1] --> trigC\n",
    "        trig_dt[2] --> trigM\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the lists are not the same length.\n",
    "    \"\"\"\n",
    "    trig_dt = []\n",
    "    \n",
    "    for df in dfs:\n",
    "        temp_dt = []\n",
    "        # Vectorization of columns\n",
    "        date = df[\"date\"].values  # date given as string.\n",
    "        time = df[\"time\"].values  # time given as string\n",
    "        corr = df[\"corr\"].values  # correction given as string\n",
    "        try:\n",
    "            if len(date) and len(time) != len(corr):\n",
    "                raise ValueError\n",
    "        except:\n",
    "            raise ValueError(\"Lists are not the same length\")\n",
    "        else:\n",
    "            for i in np.arange(0, len(corr)):  # Iterating over the vectors\n",
    "                if corr[i] == \"--------\":  # No correction needed. Appending the datetime object\n",
    "                    date_str = date[i]\n",
    "                    time_str = time[i]\n",
    "                    org_dt = datetime.strptime(\n",
    "                        date_str + \" \" + time_str, \"%Y-%b-%d %H:%M:%S.%f\")\n",
    "                    temp_dt.append(org_dt)\n",
    "                    \n",
    "                elif corr[i][0] == \"-\":  # If it's a \"-\" in front; correction is added\n",
    "                    # formatting the datetime object\n",
    "                    date_str = date[i]\n",
    "                    time_str = time[i]\n",
    "                    org_dt = datetime.strptime(\n",
    "                        date_str + \" \" + time_str, \"%Y-%b-%d %H:%M:%S.%f\")  # Original datetime\n",
    "\n",
    "                    micro_corr = int(corr[0][1:])\n",
    "\n",
    "                    # new corrected datetime. Timedelta ccounts for changes in seconds also\n",
    "                    new_dt = org_dt + timedelta(microseconds=micro_corr)\n",
    "                    temp_dt.append(new_dt)\n",
    "                else:\n",
    "                    date_str = date[i]\n",
    "                    time_str = time[i]\n",
    "                    org_dt = datetime.strptime(\n",
    "                        date_str + \" \" + time_str, \"%Y-%b-%d %H:%M:%S.%f\")  # Original datetime\n",
    "\n",
    "                    micro_corr = int(corr[0][1:])\n",
    "\n",
    "                    # new corrected datetime. Timedelta ccounts for changes in seconds also\n",
    "                    new_dt = org_dt - timedelta(microseconds=micro_corr)\n",
    "                    temp_dt.append(new_dt)\n",
    "                    \n",
    "            trig_dt.append(temp_dt)\n",
    "\n",
    "    trig_dt = np.array(trig_dt, dtype=object)\n",
    "    return trig_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "weighted-offer",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Callig corr_dt with a list containing the dataframes from ASIM\n",
    "trig_dt = corr_dt([trigB_master,trigC_master,trigM_master])\n",
    "\n",
    "#Creating array instances in trig_dt\n",
    "for i in np.arange(0,len(trig_dt)):\n",
    "    trig_dt[i] = np.asarray(trig_dt[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-contents",
   "metadata": {},
   "source": [
    "## ASIM and IPN search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "prime-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most compact algorithm for searching: np.where()\n",
    "#Storing matches in the match list. Stored as a tuple containing (datetime IPN, index trig_B)\n",
    "\n",
    "# TODO: wrap a function around it so it can take in several triggers (trigB,trigC etc..)\n",
    "def find_match(trig_arr):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ------\n",
    "    trig_arr <array> <datetime>\n",
    "    np.where returns: An array with elements from x where condition is True, and elements from y elsewhere.\n",
    "    \n",
    "    elements in matches_* contains tuples for indecies in IPN list and trig_arr. \"\"\"\n",
    "    matches_old = [] \n",
    "\n",
    "    for i in ipn_old_dt:\n",
    "        #Searching for matches that are +/- 10 seconds from the IPN trigger\n",
    "        mask = np.where((i-timedelta(seconds=10) <= trig_arr) & (trig_arr <= i + timedelta(seconds=10)))\n",
    "        if len(mask[0]) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            matches_old.append((i,mask[0]))\n",
    "\n",
    "    matches_new = []\n",
    "    for i in ipn_new_dt:\n",
    "        #Searching for matches that are +/- 10 seconds from the IPN trigger\n",
    "        mask = np.where((i-timedelta(seconds=10) <= trig_arr) & (trig_arr <= i + timedelta(seconds=10)))\n",
    "        #print(mask)\n",
    "        if len(mask[0]) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            matches_new.append((i,mask[0]))\n",
    "        \n",
    "    return (matches_old, matches_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ordinary-campbell",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_match = []\n",
    "for i in trig_dt:\n",
    "    tup_match = find_match(i)\n",
    "    master_match.append(tup_match)\n",
    "    \n",
    "#master_match[n] - refers to type of trigger (trigB, trigC, trigM)\n",
    "#master_match[n][n] - refers to old or new ipn trigger list [0]: old , [1]: new\n",
    "#master_match[n][n][n] - refers to tuple containing (ipn dt, array[index]) where index is the dt in trig_dt[n][index]\n",
    "#master_match[n][n][n][n] - selecting desired element in tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "previous-setting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([(datetime.datetime(2018, 7, 11, 17, 2, 2), array([2852])),\n",
       "   (datetime.datetime(2018, 7, 20, 14, 21, 44), array([3368, 3369])),\n",
       "   (datetime.datetime(2018, 8, 9, 13, 12, 4), array([4831])),\n",
       "   (datetime.datetime(2018, 8, 9, 14, 37, 3), array([4853])),\n",
       "   (datetime.datetime(2018, 9, 10, 3, 44, 16), array([11365])),\n",
       "   (datetime.datetime(2018, 10, 17, 20, 54, 15), array([18945])),\n",
       "   (datetime.datetime(2018, 12, 22, 20, 11, 34), array([30144])),\n",
       "   (datetime.datetime(2019, 2, 6, 3, 49, 23), array([43589])),\n",
       "   (datetime.datetime(2019, 2, 16, 11, 52, 26), array([45481])),\n",
       "   (datetime.datetime(2019, 2, 18, 19, 27, 44), array([45987])),\n",
       "   (datetime.datetime(2019, 3, 5, 13, 5, 15), array([49414, 49426])),\n",
       "   (datetime.datetime(2019, 3, 23, 21, 5, 21), array([53366])),\n",
       "   (datetime.datetime(2019, 4, 11, 9, 45, 56), array([55408])),\n",
       "   (datetime.datetime(2019, 5, 12, 14, 40, 9), array([58637])),\n",
       "   (datetime.datetime(2019, 6, 6, 1, 55, 7), array([61032])),\n",
       "   (datetime.datetime(2019, 7, 20, 14, 42, 9), array([65368])),\n",
       "   (datetime.datetime(2019, 8, 29, 19, 56, 44), array([69270])),\n",
       "   (datetime.datetime(2019, 12, 27, 17, 21, 44), array([78489])),\n",
       "   (datetime.datetime(2020, 2, 12, 10, 49, 49), array([81606])),\n",
       "   (datetime.datetime(2020, 3, 31, 17, 39, 28), array([86277])),\n",
       "   (datetime.datetime(2020, 4, 12, 6, 57, 11),\n",
       "    array([87289, 87290, 87291, 87292, 87293, 87294])),\n",
       "   (datetime.datetime(2020, 5, 21, 12, 16, 41), array([90870])),\n",
       "   (datetime.datetime(2020, 6, 19, 11, 48, 43), array([93488])),\n",
       "   (datetime.datetime(2020, 7, 16, 22, 57, 41), array([95954])),\n",
       "   (datetime.datetime(2020, 9, 3, 2, 34, 27), array([100167])),\n",
       "   (datetime.datetime(2020, 9, 25, 21, 50, 37), array([102247])),\n",
       "   (datetime.datetime(2020, 10, 13, 18, 6, 58), array([103795])),\n",
       "   (datetime.datetime(2020, 11, 14, 0, 39, 25), array([106512])),\n",
       "   (datetime.datetime(2020, 12, 27, 15, 14, 7), array([110050])),\n",
       "   (datetime.datetime(2021, 2, 22, 15, 57, 23), array([114927])),\n",
       "   (datetime.datetime(2021, 4, 5, 12, 11, 26), array([119771])),\n",
       "   (datetime.datetime(2021, 5, 22, 22, 11, 40),\n",
       "    array([124655, 124656, 124657])),\n",
       "   (datetime.datetime(2021, 6, 17, 12, 52, 54), array([127432])),\n",
       "   (datetime.datetime(2021, 6, 19, 23, 59, 25),\n",
       "    array([127675, 127676, 127677, 127678, 127679]))],\n",
       "  [(datetime.datetime(2018, 7, 20, 14, 21, 39), array([3368])),\n",
       "   (datetime.datetime(2018, 8, 7, 2, 19, 37, 824000), array([4561])),\n",
       "   (datetime.datetime(2018, 9, 10, 3, 44, 15, 315000), array([11365])),\n",
       "   (datetime.datetime(2018, 11, 3, 4, 22, 30), array([21262])),\n",
       "   (datetime.datetime(2018, 12, 22, 20, 11, 34, 563000), array([30144])),\n",
       "   (datetime.datetime(2019, 3, 5, 13, 5, 15, 900000), array([49414, 49426])),\n",
       "   (datetime.datetime(2019, 3, 20, 1, 14, 15, 464000), array([52522, 52537])),\n",
       "   (datetime.datetime(2019, 3, 23, 21, 5, 19, 785000), array([53366])),\n",
       "   (datetime.datetime(2019, 4, 4, 7, 1, 14, 501000), array([55094])),\n",
       "   (datetime.datetime(2019, 6, 6, 1, 55, 3, 800000), array([61032])),\n",
       "   (datetime.datetime(2019, 7, 6, 12, 40, 50), array([64102])),\n",
       "   (datetime.datetime(2019, 7, 20, 14, 42, 9), array([65368])),\n",
       "   (datetime.datetime(2019, 12, 27, 17, 21, 40, 487000), array([78489])),\n",
       "   (datetime.datetime(2020, 2, 12, 10, 49, 44, 544000), array([81606])),\n",
       "   (datetime.datetime(2020, 4, 12, 6, 57, 11, 947000),\n",
       "    array([87289, 87290, 87291, 87292, 87293, 87294, 87295])),\n",
       "   (datetime.datetime(2020, 5, 21, 12, 16, 41), array([90870])),\n",
       "   (datetime.datetime(2020, 6, 19, 11, 48, 42, 574000), array([93488])),\n",
       "   (datetime.datetime(2020, 7, 16, 22, 57, 38, 337000), array([95954])),\n",
       "   (datetime.datetime(2020, 9, 3, 2, 34, 27), array([100167])),\n",
       "   (datetime.datetime(2020, 9, 25, 21, 50, 37), array([102247])),\n",
       "   (datetime.datetime(2020, 12, 23, 17, 58, 6, 750000), array([109860])),\n",
       "   (datetime.datetime(2020, 12, 27, 15, 14, 6, 705000), array([110050])),\n",
       "   (datetime.datetime(2021, 4, 11, 13, 32, 31, 65000), array([120380])),\n",
       "   (datetime.datetime(2021, 6, 19, 23, 59, 25),\n",
       "    array([127675, 127676, 127677, 127678, 127679])),\n",
       "   (datetime.datetime(2021, 7, 1, 20, 1, 8, 875000), array([128850])),\n",
       "   (datetime.datetime(2021, 7, 2, 19, 7, 6, 870000),\n",
       "    array([128952, 128953, 128954])),\n",
       "   (datetime.datetime(2021, 7, 24, 20, 14, 9), array([131382])),\n",
       "   (datetime.datetime(2021, 9, 3, 17, 26, 56, 349000), array([135871])),\n",
       "   (datetime.datetime(2021, 11, 18, 23, 38, 13), array([143610])),\n",
       "   (datetime.datetime(2021, 12, 11, 13, 9, 59),\n",
       "    array([145972, 145973, 145974, 145975]))]),\n",
       " ([(datetime.datetime(2018, 12, 27, 6, 16, 59), array([11202])),\n",
       "   (datetime.datetime(2019, 1, 17, 8, 50, 43), array([12923])),\n",
       "   (datetime.datetime(2019, 4, 20, 23, 32, 24), array([20754])),\n",
       "   (datetime.datetime(2019, 5, 1, 5, 23, 21), array([21437])),\n",
       "   (datetime.datetime(2019, 6, 15, 14, 42, 25), array([23861])),\n",
       "   (datetime.datetime(2019, 6, 28, 4, 23, 39), array([24588])),\n",
       "   (datetime.datetime(2019, 8, 13, 12, 0, 45), array([27017])),\n",
       "   (datetime.datetime(2019, 8, 25, 21, 4, 56), array([27707, 27708])),\n",
       "   (datetime.datetime(2019, 9, 6, 1, 4, 53), array([28416])),\n",
       "   (datetime.datetime(2019, 10, 1, 6, 41, 50), array([30103])),\n",
       "   (datetime.datetime(2019, 10, 4, 18, 7, 2), array([30481])),\n",
       "   (datetime.datetime(2019, 11, 19, 10, 41, 9), array([36246])),\n",
       "   (datetime.datetime(2019, 12, 21, 20, 39, 13),\n",
       "    array([37531, 37532, 37533, 37534, 37535])),\n",
       "   (datetime.datetime(2019, 12, 27, 17, 21, 44), array([38107])),\n",
       "   (datetime.datetime(2020, 1, 11, 15, 11, 8), array([40031])),\n",
       "   (datetime.datetime(2020, 1, 22, 5, 18, 20), array([41477])),\n",
       "   (datetime.datetime(2020, 2, 24, 9, 58, 44), array([45445])),\n",
       "   (datetime.datetime(2020, 4, 15, 8, 48, 5), array([53009])),\n",
       "   (datetime.datetime(2020, 4, 23, 13, 54, 11), array([54213])),\n",
       "   (datetime.datetime(2020, 5, 25, 14, 40, 28), array([57169])),\n",
       "   (datetime.datetime(2020, 6, 5, 18, 17, 42), array([58763])),\n",
       "   (datetime.datetime(2020, 8, 12, 11, 55, 28), array([66374, 66375])),\n",
       "   (datetime.datetime(2020, 9, 3, 21, 38, 58), array([69085])),\n",
       "   (datetime.datetime(2020, 9, 7, 18, 51, 11), array([69619])),\n",
       "   (datetime.datetime(2020, 9, 15, 3, 27, 15), array([70629])),\n",
       "   (datetime.datetime(2020, 9, 23, 17, 57, 43), array([71775])),\n",
       "   (datetime.datetime(2020, 11, 4, 0, 0, 56), array([75674])),\n",
       "   (datetime.datetime(2020, 11, 9, 2, 31, 5), array([76066])),\n",
       "   (datetime.datetime(2021, 1, 2, 20, 38, 11), array([79197, 79198])),\n",
       "   (datetime.datetime(2021, 4, 24, 8, 1, 56), array([89577])),\n",
       "   (datetime.datetime(2021, 6, 7, 21, 39, 20), array([92671, 92672, 92673]))],\n",
       "  [(datetime.datetime(2018, 12, 27, 6, 16, 59, 317000), array([11202])),\n",
       "   (datetime.datetime(2019, 1, 17, 8, 50, 42, 572000), array([12923])),\n",
       "   (datetime.datetime(2019, 4, 20, 23, 32, 24, 838000), array([20754])),\n",
       "   (datetime.datetime(2019, 5, 1, 5, 23, 15), array([21437])),\n",
       "   (datetime.datetime(2019, 9, 6, 1, 4, 49), array([28416])),\n",
       "   (datetime.datetime(2019, 10, 1, 6, 41, 46, 867000), array([30103])),\n",
       "   (datetime.datetime(2019, 10, 4, 18, 7, 2), array([30481])),\n",
       "   (datetime.datetime(2019, 12, 21, 20, 39, 9, 647000),\n",
       "    array([37531, 37532, 37533, 37534, 37535])),\n",
       "   (datetime.datetime(2019, 12, 27, 17, 21, 40, 487000), array([38107])),\n",
       "   (datetime.datetime(2020, 1, 11, 15, 11, 8), array([40031])),\n",
       "   (datetime.datetime(2020, 1, 22, 5, 18, 19, 202000), array([41477])),\n",
       "   (datetime.datetime(2020, 2, 24, 9, 58, 44), array([45445])),\n",
       "   (datetime.datetime(2020, 3, 3, 2, 34, 23), array([46460])),\n",
       "   (datetime.datetime(2020, 4, 15, 8, 48, 1, 401000), array([53009])),\n",
       "   (datetime.datetime(2020, 4, 23, 13, 54, 11), array([54213])),\n",
       "   (datetime.datetime(2020, 4, 25, 16, 49, 49, 665000), array([54454])),\n",
       "   (datetime.datetime(2020, 5, 25, 14, 40, 18, 758000), array([57169])),\n",
       "   (datetime.datetime(2020, 6, 5, 18, 17, 38, 111000), array([58763])),\n",
       "   (datetime.datetime(2020, 9, 3, 21, 38, 56, 784000), array([69085])),\n",
       "   (datetime.datetime(2020, 9, 7, 18, 51, 11), array([69619])),\n",
       "   (datetime.datetime(2020, 9, 15, 3, 27, 14, 927000), array([70629])),\n",
       "   (datetime.datetime(2020, 9, 23, 17, 57, 43), array([71775])),\n",
       "   (datetime.datetime(2020, 11, 4, 0, 0, 56), array([75674])),\n",
       "   (datetime.datetime(2020, 11, 9, 2, 31, 5), array([76066])),\n",
       "   (datetime.datetime(2020, 11, 20, 9, 38, 46, 777000), array([77007, 77008])),\n",
       "   (datetime.datetime(2021, 1, 2, 20, 38, 10, 398000), array([79197, 79198])),\n",
       "   (datetime.datetime(2021, 1, 8, 10, 10, 9, 440000), array([79637])),\n",
       "   (datetime.datetime(2021, 4, 24, 8, 1, 56), array([89577])),\n",
       "   (datetime.datetime(2021, 8, 2, 20, 8, 6, 490000), array([97569, 97570])),\n",
       "   (datetime.datetime(2021, 11, 20, 23, 5, 17, 928000),\n",
       "    array([109193, 109194])),\n",
       "   (datetime.datetime(2021, 12, 19, 21, 49, 44, 472000), array([110878])),\n",
       "   (datetime.datetime(2021, 12, 23, 14, 29, 26), array([111080]))]),\n",
       " ([(datetime.datetime(2018, 6, 22, 13, 52, 42), array([2506])),\n",
       "   (datetime.datetime(2018, 7, 4, 5, 36, 42), array([5617])),\n",
       "   (datetime.datetime(2018, 7, 15, 18, 7, 5), array([10851, 10852])),\n",
       "   (datetime.datetime(2018, 8, 9, 14, 37, 3),\n",
       "    array([22312, 22313, 22314, 22315, 22316])),\n",
       "   (datetime.datetime(2018, 9, 23, 22, 20, 30), array([41349])),\n",
       "   (datetime.datetime(2018, 9, 24, 15, 22, 22), array([41532])),\n",
       "   (datetime.datetime(2018, 10, 11, 4, 21, 10), array([49936])),\n",
       "   (datetime.datetime(2018, 11, 3, 1, 6, 45), array([57091])),\n",
       "   (datetime.datetime(2018, 11, 26, 9, 54, 8), array([65822])),\n",
       "   (datetime.datetime(2019, 3, 4, 8, 54, 35), array([96296, 96297])),\n",
       "   (datetime.datetime(2019, 7, 26, 8, 52, 44),\n",
       "    array([130160, 130161, 130190, 130191, 130192])),\n",
       "   (datetime.datetime(2019, 9, 3, 17, 19, 36), array([139565, 139566])),\n",
       "   (datetime.datetime(2019, 9, 5, 23, 38, 28), array([140661])),\n",
       "   (datetime.datetime(2020, 5, 24, 13, 52, 53), array([191072, 191073])),\n",
       "   (datetime.datetime(2020, 7, 9, 3, 31, 16), array([200906, 200907, 200908])),\n",
       "   (datetime.datetime(2020, 9, 8, 21, 15, 41),\n",
       "    array([214995, 214996, 214997, 214998])),\n",
       "   (datetime.datetime(2020, 9, 9, 23, 17, 10), array([215937])),\n",
       "   (datetime.datetime(2020, 11, 8, 15, 28, 23), array([229420])),\n",
       "   (datetime.datetime(2020, 11, 28, 23, 57),\n",
       "    array([232490, 232491, 232492, 232493, 232494]))],\n",
       "  [(datetime.datetime(2018, 6, 22, 13, 52, 42, 163000), array([2506])),\n",
       "   (datetime.datetime(2018, 7, 15, 18, 7, 4, 869000), array([10851, 10852])),\n",
       "   (datetime.datetime(2018, 8, 14, 12, 6, 3, 641000), array([24543])),\n",
       "   (datetime.datetime(2018, 9, 23, 22, 20, 30, 530000), array([41349])),\n",
       "   (datetime.datetime(2018, 9, 24, 15, 22, 22), array([41532])),\n",
       "   (datetime.datetime(2018, 11, 3, 1, 6, 43, 48000), array([57091])),\n",
       "   (datetime.datetime(2018, 11, 26, 9, 54, 8), array([65822])),\n",
       "   (datetime.datetime(2019, 1, 29, 12, 55, 41, 395000), array([83799])),\n",
       "   (datetime.datetime(2019, 2, 22, 7, 29, 33, 222000), array([90976])),\n",
       "   (datetime.datetime(2019, 3, 4, 8, 54, 34, 747000), array([96296, 96297])),\n",
       "   (datetime.datetime(2019, 8, 31, 16, 38, 35, 674000),\n",
       "    array([137680, 137681])),\n",
       "   (datetime.datetime(2019, 9, 3, 17, 19, 36), array([139565, 139566])),\n",
       "   (datetime.datetime(2019, 9, 5, 23, 38, 27, 913000), array([140661])),\n",
       "   (datetime.datetime(2019, 11, 19, 6, 16, 1, 287000), array([163236])),\n",
       "   (datetime.datetime(2020, 7, 9, 3, 30, 56, 28000),\n",
       "    array([200903, 200904, 200905])),\n",
       "   (datetime.datetime(2020, 9, 3, 15, 32, 4), array([212638, 212639])),\n",
       "   (datetime.datetime(2020, 9, 8, 21, 15, 40, 150000),\n",
       "    array([214994, 214995, 214996, 214997, 214998])),\n",
       "   (datetime.datetime(2020, 9, 9, 23, 17, 10), array([215937])),\n",
       "   (datetime.datetime(2020, 11, 8, 15, 28, 22, 499000), array([229420])),\n",
       "   (datetime.datetime(2021, 5, 29, 23, 53, 36, 74000),\n",
       "    array([264732, 264733])),\n",
       "   (datetime.datetime(2021, 8, 26, 7, 1, 45),\n",
       "    array([280492, 280493, 280494, 280495])),\n",
       "   (datetime.datetime(2021, 12, 4, 3, 27, 33, 877000), array([306736]))])]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-steal",
   "metadata": {},
   "source": [
    "## Branching\n",
    "Branching into matches in old ipn dataset and matches in new ipn dataset.\n",
    "\n",
    "Export into a dataframe containing IPN dt trigger | trig* dt trigger\n",
    "\n",
    "\n",
    "\n",
    "How to access data from master_match:\n",
    "\n",
    "\n",
    "- master_match[n] - refers to type of trigger (trigB, trigC, trigM)\n",
    "\n",
    "- master_match[n][n] - refers to old or new ipn trigger list [0]: old , [1]: new\n",
    "\n",
    "- master_match[n][n][n] - refers to tuple containing (ipn dt, array[index]) where index is the dt in trig_dt[n][index]\n",
    "\n",
    "- master_match[n][n][n][n] - selecting desired element in tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "sensitive-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_trigB_old = master_match[0][0]\n",
    "match_trigB_new = master_match[0][1]\n",
    "\n",
    "match_trigC_old = master_match[1][0]\n",
    "match_trigC_new = master_match[1][1]\n",
    "\n",
    "match_trigM_old = master_match[2][0]\n",
    "match_trigM_new = master_match[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "alleged-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe for ipn old and ipn new. Columns should be ipn dt trigger, trigB, trigC,trigM\n",
    "# if there isnt a trigger in the respected trigger, the entry should be None\n",
    "# But first. Produce df of ipn --> trigB, trigC, trigM [indecies]. Just indecies because there are several in some triggers\n",
    "\n",
    "trigB_old_dict = {\"ipn old dt\":[], \"trigB index\": []}\n",
    "for i in match_trigB_old:\n",
    "    trigB_old_dict[\"ipn old dt\"].append(i[0])\n",
    "    trigB_old_dict[\"trigB index\"].append(i[1])\n",
    "trigB_old_df = pd.DataFrame(trigB_old_dict)\n",
    "\n",
    "trigB_new_dict = {\"ipn new dt\":[], \"trigB index\": []}\n",
    "for i in match_trigB_new:\n",
    "    trigB_new_dict[\"ipn new dt\"].append(i[0])\n",
    "    trigB_new_dict[\"trigB index\"].append(i[1])\n",
    "trigB_new_df = pd.DataFrame(trigB_new_dict)\n",
    "\n",
    "trigC_old_dict = {\"ipn old dt\":[], \"trigC index\": []}\n",
    "for i in match_trigC_old:\n",
    "    trigC_old_dict[\"ipn old dt\"].append(i[0])\n",
    "    trigC_old_dict[\"trigC index\"].append(i[1])\n",
    "trigC_old_df = pd.DataFrame(trigC_old_dict)\n",
    "\n",
    "trigC_new_dict = {\"ipn new dt\":[], \"trigC index\": []}\n",
    "for i in match_trigC_new:\n",
    "    trigC_new_dict[\"ipn new dt\"].append(i[0])\n",
    "    trigC_new_dict[\"trigC index\"].append(i[1])\n",
    "trigC_new_df = pd.DataFrame(trigC_old_dict)\n",
    "\n",
    "trigM_old_dict = {\"ipn old dt\":[], \"trigM index\": []}\n",
    "for i in match_trigM_old:\n",
    "    trigM_old_dict[\"ipn old dt\"].append(i[0])\n",
    "    trigM_old_dict[\"trigM index\"].append(i[1])\n",
    "trigM_old_df = pd.DataFrame(trigM_old_dict)\n",
    "\n",
    "trigM_new_dict = {\"ipn new dt\":[], \"trigM index\": []}\n",
    "for i in match_trigM_new:\n",
    "    trigM_new_dict[\"ipn new dt\"].append(i[0])\n",
    "    trigM_new_dict[\"trigM index\"].append(i[1])\n",
    "trigM_new_df = pd.DataFrame(trigM_new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "floral-appendix",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigB_old_indecies = trigB_old_df[\"trigB index\"].values\n",
    "trigB_old_dts = index_to_dt(0,trigB_old_indecies)\n",
    "\n",
    "trigC_old_indecies = trigC_old_df[\"trigC index\"].values\n",
    "trigC_old_dts = index_to_dt(1,trigC_old_indecies)\n",
    "\n",
    "trigM_old_indecies = trigM_old_df[\"trigM index\"].values\n",
    "trigM_old_dts = index_to_dt(2,trigM_old_indecies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "radical-involvement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-mistake",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-beast",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "indie-carolina",
   "metadata": {},
   "source": [
    "## Exporting matches from IPN and IPN_supp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm for saving the matches in a file:\n",
    "trigB_match_file = open(\"trigB_match.txt\", \"w\")\n",
    "for i in np.arange(0,len(trigB_match)):\n",
    "    for row in trigB_match[i]:\n",
    "        np.savetxt(trigB_match_file, (str(row[0]), row[1]) , fmt='%s')\n",
    "\n",
    "trigB_match_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-central",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-monroe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "thousand-addiction",
   "metadata": {},
   "source": [
    "### Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-catholic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to filter duplicates\n",
    "def filter_duplicates(l1,l2):\n",
    "    \"\"\"\n",
    "    l1 --> ipn_old match\n",
    "    l2 --> ipn_new match\n",
    "    \n",
    "    Converting l1,l2 to np.arrays to handle the filtering. (might not be neccecery)\n",
    "    Checking if dt object in l2 is not present in l1. If so, then add the tuple to l1\n",
    "    Parameters: l1,l2 <list> containing tuples\n",
    "    ----\n",
    "    Returns: l3 <array> of dt objects that are not in \"\"\"\n",
    "    \n",
    "    found_time = []\n",
    "    indecies = []\n",
    "    for index, i in enumerate(ipn_new_updated[0]):\n",
    "        if i not in ipn_old_updated:\n",
    "            found_time.append(i)\n",
    "            indecies.append(index)\n",
    "        \n",
    "    found_time = np.asarray(found_time)\n",
    "    indecies = np.asarray(indecies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-loading",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
