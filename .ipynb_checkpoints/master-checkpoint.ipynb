{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "honey-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-plastic",
   "metadata": {},
   "source": [
    "## Objective:\n",
    "-Import the datasets. Still need the extra dataset. Includes triggers up to dec. 21\n",
    "\n",
    "-datetime helpful docs: https://www.w3schools.com/python/python_datetime.asp\n",
    "\n",
    "-Create algorithm that searches for matches that are +/- 10 second apart and remove the rest (maybe two seconds)\n",
    "\n",
    "-First match found at 2018, 7, 11, 17, 2, 4\n",
    "\n",
    "-Do spectral analysis on the data (counts/bin)\n",
    "### Notes from stackoverflow:\n",
    "np.where() implementation: https://stackoverflow.com/questions/35714902/find-location-of-pair-of-elements-in-two-arrays-in-numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-steal",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lucky-cooler",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing and redefining the dataframes\n",
    "#Not loading the trigger list for 2021 yet\n",
    "\n",
    "ipn_data = pd.read_csv(\"./IPN/trigIPN.csv\", sep=\"|\")\n",
    "ipn = pd.DataFrame(ipn_data)\n",
    "ipn.drop(columns= ['Unnamed: 0', 'Unnamed: 2'], axis=1, inplace=True) #dropping unwanted columns\n",
    "ipn.rename(columns={ipn.columns[0]:\"time\"}, inplace = True)\n",
    "\n",
    "trigB_data = pd.read_csv(\"./ASIM/trigB.txt\", sep = \"\\s+|\\t+|\\s+\\t+|\\t+\\s+\", engine=\"python\")\n",
    "trigB = pd.DataFrame(trigB_data)\n",
    "trigB.drop(columns=[\"######\"], inplace=True) #dropping unwanted columns\n",
    "trigB.rename(columns={\"yyyy-MMM-dd\":\"date\",\"HH:mm:ss.SSSSSS\": \"time\", \"Corr\":\"corr\"}, inplace=True) #renaming columns\n",
    "\n",
    "#trigB_21_data = pd.read_csv(\"./ASIM/trigB_2021.txt\", sep = \"\\s+|\\t+|\\s+\\t+|\\t+\\s+\", engine=\"python\")\n",
    "#trigB_21 = pd.DataFrame(trigB_21_data)\n",
    "#trigB_21.drop(columns=[\"######\"], inplace=True)\n",
    "#trigB_21.rename(columns={\"yyyy-MMM-dd\":\"date\",\"HH:mm:ss.SSSSSS\": \"time\" }, inplace=True)\n",
    "\n",
    "#trigC_data = pd.read_csv(\"./ASIM/trigC.txt\", sep = \"\\s+|\\t+|\\s+\\t+|\\t+\\s+\", engine=\"python\")\n",
    "#trigC = pd.DataFrame(trigC_data)\n",
    "#trigC.drop(columns=[\"######\"], inplace=True)\n",
    "#trigC.rename(columns={\"yyyy-MMM-dd\":\"date\",\"HH:mm:ss.SSSSSS\": \"time\" ,\"Corr\":\"corr\"}, inplace=True)\n",
    "\n",
    "#trigC_21_data = pd.read_csv(\"./ASIM/trigC_2021.txt\", sep = \"\\s+|\\t+|\\s+\\t+|\\t+\\s+\", engine=\"python\")\n",
    "#trigC_21 = pd.DataFrame(trigC_21_data)\n",
    "#trigC_21.drop(columns=[\"######\"], inplace=True)\n",
    "#trigC_21.rename(columns={\"yyyy-MMM-dd\":\"date\",\"HH:mm:ss.SSSSSS\": \"time\" }, inplace=True)\n",
    "\n",
    "#trigM_data = pd.read_csv(\"./ASIM/trigM.txt\", sep = \"\\s+|\\t+|\\s+\\t+|\\t+\\s+\", engine=\"python\")\n",
    "#trigM = pd.DataFrame(trigM_data)\n",
    "#trigM.drop(columns=[\"######\"], inplace=True)\n",
    "#trigM.rename(columns={\"yyyy-MMM-dd\":\"date\",\"HH:mm:ss.SSSSSS\": \"time\", \"Corr\":\"corr\" }, inplace=True)\n",
    "\n",
    "#trigM_21_data = pd.read_csv(\"./ASIM/trigM_2021.txt\", sep = \"\\s+|\\t+|\\s+\\t+|\\t+\\s+\", engine=\"python\")\n",
    "#trigM_21 = pd.DataFrame(trigM_21_data)\n",
    "#trigM_21.drop(columns=[\"######\"], inplace=True)\n",
    "#trigM_21.rename(columns={\"yyyy-MMM-dd\":\"date\",\"HH:mm:ss.SSSSSS\": \"time\" }, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-harvey",
   "metadata": {},
   "source": [
    "### Importing supplementary IPN triggers \n",
    "Latest ASIM trigger: 2021, 3, 20, 22, 51, 59\n",
    "\n",
    "Latest IPN trigger: 2021, 6, 27, 19, 31, 37 --> Extending this until the end of the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sharing-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipn_supp_data = pd.read_csv(\"./IPN/ipn_supp.txt\",sep = \"\\s+|\\t+|\\s+\\t+|\\t+\\s+\", engine=\"python\")\n",
    "ipn_supp = pd.DataFrame(ipn_supp_data)\n",
    "#Dropping unwanted columns\n",
    "ipn_supp = ipn_supp.drop([\"GRB_name_Fermi\",'ra(°J2000)', 'decl(°J2000)',\n",
    "       'pos_error(°1-sigma)', 'T90(s)', 'T90_error(s)', 'T90_start(UTC)',\n",
    "       'fluence(erg/cm^2)', 'fluence_error(erg/cm^2)', 'redshift', 'T100(s)',\n",
    "       'GBM_located', 'mjd(T0day)'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "blond-briefing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization of names and times from dataframe. Zero-padding hour in times\n",
    "names = ipn_supp[\"GRB_name\"].values\n",
    "times = ipn_supp[\"T0(UTC)\"].values  # Hour is not zero padded.\n",
    "\n",
    "# Zero padding times\n",
    "def zero_padding(non_zero_padd):\n",
    "    new_times = []\n",
    "\n",
    "    for time in non_zero_padd:\n",
    "        if time.index(\":\") < 2:\n",
    "            time = \"0\" + time\n",
    "            new_times.append(time)\n",
    "        else:\n",
    "            new_times.append(time)\n",
    "    return new_times\n",
    "\n",
    "new_times = zero_padding(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adaptive-happening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining datetime objects.\n",
    "# Problem: some of the times have microsecond resolution while other dont\n",
    "new_ipn = []\n",
    "if len(names) == len(new_times):\n",
    "    for i in np.arange(0, len(names)):\n",
    "        date = names[i][3:9]\n",
    "        time = new_times[i]\n",
    "\n",
    "        # Managing the microsecond resolution\n",
    "        if len(time) > 8:\n",
    "            dt_obj = datetime.strptime(date + \" \" + time, \"%y%m%d %H:%M:%S.%f\")\n",
    "        else:\n",
    "            dt_obj = datetime.strptime(date + \" \" + time, \"%y%m%d %H:%M:%S\")\n",
    "            \n",
    "        new_ipn.append(dt_obj)\n",
    "else:\n",
    "    print(\"Unequal length of arrays\")\n",
    "    \n",
    "#Earliest entry in the ASIM data 2018,6,1,12,46,8\n",
    "# Including only entries up to 1142. That means entries in IPN from 2018, 5, 29, 8, 29, 14\n",
    "\n",
    "new_ipn = np.asarray(new_ipn[:1142][::-1]) #Slicing the list. Returning in reverse order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dynamic-municipality",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Old IPN datetime list\n",
    "ipn_dt_temp = []\n",
    "ipn_dict = ipn.to_dict(\"records\")\n",
    "for row in ipn_dict:\n",
    "    datetime_str = row[\"time\"]\n",
    "    datetime_obj = datetime.strptime(datetime_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    ipn_dt_temp.append(datetime_obj)\n",
    "    \n",
    "old_ipn = np.asarray(ipn_dt_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-bracelet",
   "metadata": {},
   "source": [
    "## Vectorization and ASIM datetime correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "turkish-radical",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Function for correcting date and time in ASIM data. Returning matrix that contains datetime objects\n",
    "# Method for correcting time\n",
    "# 1. Retrive the time from time column\n",
    "# 2. Isolate the microsecond time from that and cast it to an int\n",
    "# 3. Retrive the correction time from Corr column and cast it to an int\n",
    "# 4. Subtract correction time from time and cast it to an string\n",
    "# 5. Insert the corrected time\n",
    "\n",
    "# PROBLEM: ONLY ONE DATETIME IS ADDED\n",
    "\n",
    "def corr_dt(dfs):  # Correcting times from ASIM data\n",
    "\n",
    "    \"\"\"   This function corrects the time from ASIM data.\n",
    "    The correction is done by subtracting the correction from the original time.\n",
    "    The correction is given as a string.\n",
    "    The function takes a list of dataframes as input.\n",
    "    The function returns a list of arrays containing the corrected datetime objects.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dfs : list of dataframes\n",
    "        The dataframes containing the data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    trig_dt : list of arrays\n",
    "        The corrected datetime objects.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the lists are not the same length.\n",
    "    \"\"\"\n",
    "    trig_dt = []\n",
    "    \n",
    "    for df in dfs:\n",
    "        temp_dt = []\n",
    "        # Vectorization of columns\n",
    "        date = df[\"date\"].values  # date given as string.\n",
    "        time = df[\"time\"].values  # time given as string\n",
    "        corr = df[\"corr\"].values  # correction given as string\n",
    "        try:\n",
    "            if len(date) and len(time) != len(corr):\n",
    "                raise ValueError\n",
    "        except:\n",
    "            raise ValueError(\"Lists are not the same length\")\n",
    "        else:\n",
    "            for i in np.arange(0, len(corr)):  # Iterating over the vectors\n",
    "                if corr[i] == \"--------\":  # No correction needed. Appending the datetime object\n",
    "                    date_str = date[i]\n",
    "                    time_str = time[i]\n",
    "                    org_dt = datetime.strptime(\n",
    "                        date_str + \" \" + time_str, \"%Y-%b-%d %H:%M:%S.%f\")\n",
    "                    temp_dt.append(org_dt)\n",
    "                    \n",
    "                elif corr[i][0] == \"-\":  # If it's a \"-\" in front; correction is added\n",
    "                    # formatting the datetime object\n",
    "                    date_str = date[i]\n",
    "                    time_str = time[i]\n",
    "                    org_dt = datetime.strptime(\n",
    "                        date_str + \" \" + time_str, \"%Y-%b-%d %H:%M:%S.%f\")  # Original datetime\n",
    "\n",
    "                    micro_corr = int(corr[0][1:])\n",
    "\n",
    "                    # new corrected datetime. Timedelta ccounts for changes in seconds also\n",
    "                    new_dt = org_dt + timedelta(microseconds=micro_corr)\n",
    "                    temp_dt.append(new_dt)\n",
    "                else:\n",
    "                    date_str = date[i]\n",
    "                    time_str = time[i]\n",
    "                    org_dt = datetime.strptime(\n",
    "                        date_str + \" \" + time_str, \"%Y-%b-%d %H:%M:%S.%f\")  # Original datetime\n",
    "\n",
    "                    micro_corr = int(corr[0][1:])\n",
    "\n",
    "                    # new corrected datetime. Timedelta ccounts for changes in seconds also\n",
    "                    new_dt = org_dt - timedelta(microseconds=micro_corr)\n",
    "                    temp_dt.append(new_dt)\n",
    "                    \n",
    "            trig_dt.append(temp_dt)\n",
    "\n",
    "    trig_dt = np.array(trig_dt)\n",
    "    return trig_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "theoretical-reducing",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#Callig corr_dt with a list containing the dataframes from ASIM\n",
    "trig_dt = corr_dt([trigB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-jimmy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-contents",
   "metadata": {},
   "source": [
    "## Algorithm for match between ASIM and IPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "driving-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most compact algorithm for searching using np.where()\n",
    "#Storing matches in the match list. Stored as a tuple containing (datetime IPN, index trig_B)\n",
    "\n",
    "# TODO: wrap a function around it so it can take in several triggers (trigB,trigC etc..)\n",
    "matches_old = []\n",
    "\n",
    "for i in old_ipn:\n",
    "    #Searching for matches that are +/- 10 seconds from the IPN trigger\n",
    "    mask = np.where((i-timedelta(seconds=10) <= trig_dt) & (trig_dt <= i + timedelta(seconds=10)))\n",
    "    if mask[1].size == 0:\n",
    "        continue\n",
    "    else:\n",
    "        matches_old.append((i,mask[1][0]))\n",
    "\n",
    "matches_new = []\n",
    "for i in new_ipn:\n",
    "    #Searching for matches that are +/- 10 seconds from the IPN trigger\n",
    "    mask = np.where((i-timedelta(seconds=10) <= trig_dt) & (trig_dt <= i + timedelta(seconds=10)))\n",
    "    if mask[1].size == 0:\n",
    "        continue\n",
    "    else:\n",
    "        matches_new.append((i,mask[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "happy-athens",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 22)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matches_old),len(matches_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "statewide-qatar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-11 17:02:02\n",
      "2018-07-20 14:21:44\n",
      "2018-08-09 13:12:04\n",
      "2018-08-09 14:37:03\n",
      "2018-09-10 03:44:16\n",
      "2018-10-17 20:54:15\n",
      "2018-12-22 20:11:34\n",
      "2019-02-06 03:49:23\n",
      "2019-02-16 11:52:26\n",
      "2019-02-18 19:27:44\n",
      "2019-03-05 13:05:15\n",
      "2019-03-23 21:05:21\n",
      "2019-04-11 09:45:56\n",
      "2019-05-12 14:40:09\n",
      "2019-06-06 01:55:07\n",
      "2019-07-20 14:42:09\n",
      "2019-08-29 19:56:44\n",
      "2019-12-27 17:21:44\n",
      "2020-02-12 10:49:49\n",
      "2020-03-31 17:39:28\n",
      "2020-04-12 06:57:11\n",
      "2020-05-21 12:16:41\n",
      "2020-06-19 11:48:43\n",
      "2020-07-16 22:57:41\n",
      "2020-09-03 02:34:27\n",
      "2020-09-25 21:50:37\n",
      "2020-10-13 18:06:58\n",
      "2020-11-14 00:39:25\n",
      "2020-12-27 15:14:07\n",
      "2021-02-22 15:57:23\n"
     ]
    }
   ],
   "source": [
    "for tup in matches_old:\n",
    "    print(tup[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "juvenile-means",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_new_update = []\n",
    "for tup in matches_new:\n",
    "    dt = tup[0].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    matches_new_update.append((dt,tup[1]))\n",
    "    \n",
    "matches_old_update = []\n",
    "for tup in matches_old:\n",
    "    dt = tup[0].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    matches_old_update.append((dt,tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "unable-mount",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true 0\n",
      "true 1\n",
      "true 2\n",
      "true 3\n",
      "true 4\n",
      "true 5\n",
      "true 6\n",
      "true 7\n",
      "true 8\n",
      "true 9\n",
      "true 10\n",
      "true 11\n",
      "true 12\n",
      "true 13\n",
      "true 14\n",
      "true 15\n",
      "true 16\n",
      "true 17\n",
      "true 18\n",
      "true 19\n",
      "true 20\n",
      "true 21\n",
      "true 22\n",
      "true 23\n",
      "true 24\n",
      "true 25\n",
      "true 26\n",
      "true 27\n",
      "true 28\n",
      "true 29\n"
     ]
    }
   ],
   "source": [
    "for index, row in enumerate(matches_old_update):\n",
    "    if row[0] == "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "pursuant-duncan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = []\n",
    "for tup in matches_new_update:\n",
    "    if tup[0] not in matches_old:\n",
    "        check.append(tup)\n",
    "len(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "logical-grounds",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2018-07-11 17:02:02',\n",
       " '2018-07-20 14:21:44',\n",
       " '2018-08-09 13:12:04',\n",
       " '2018-08-09 14:37:03',\n",
       " '2018-09-10 03:44:16',\n",
       " '2018-10-17 20:54:15',\n",
       " '2018-12-22 20:11:34',\n",
       " '2019-02-06 03:49:23',\n",
       " '2019-02-16 11:52:26',\n",
       " '2019-02-18 19:27:44',\n",
       " '2019-03-05 13:05:15',\n",
       " '2019-03-23 21:05:21',\n",
       " '2019-04-11 09:45:56',\n",
       " '2019-05-12 14:40:09',\n",
       " '2019-06-06 01:55:07',\n",
       " '2019-07-20 14:42:09',\n",
       " '2019-08-29 19:56:44',\n",
       " '2019-12-27 17:21:44',\n",
       " '2020-02-12 10:49:49',\n",
       " '2020-03-31 17:39:28',\n",
       " '2020-04-12 06:57:11',\n",
       " '2020-05-21 12:16:41',\n",
       " '2020-06-19 11:48:43',\n",
       " '2020-07-16 22:57:41',\n",
       " '2020-09-03 02:34:27',\n",
       " '2020-09-25 21:50:37',\n",
       " '2020-10-13 18:06:58',\n",
       " '2020-11-14 00:39:25',\n",
       " '2020-12-27 15:14:07',\n",
       " '2021-02-22 15:57:23']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches_old_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-playback",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-charge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-produce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-wound",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_old = matches_old\n",
    "match_new = matches_new\n",
    "\n",
    "#Finding the dt which are in the new but not in the old\n",
    "add_match = []\n",
    "\n",
    "for dt in match_new:\n",
    "    if dt not in match_old:\n",
    "        add_match.append(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(add_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-slide",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_old = matches\n",
    "matches_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-clearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_new = matches\n",
    "matches_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_not_in_old = []\n",
    "for dt in matches_new:\n",
    "    if dt[0] not in matches_old:\n",
    "        matches_not_in_old.append(dt)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_not_in_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_old"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-addiction",
   "metadata": {},
   "source": [
    "### Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crude way of searching for a match\n",
    "#Extracting year,month,day,hour,minute from the ipn and trig array. Narrowing the search!\n",
    "def extract_datetime(dt_object):\n",
    "    year = dt_object.year\n",
    "    month = dt_object.month\n",
    "    day = dt_object.day\n",
    "    hour = dt_object.hour\n",
    "    minute = dt_object.minute\n",
    "    return year,month,day,hour,minute\n",
    "\n",
    "# Retriving datetimeobjects that fits the criteria; same year,month,day,hour,minute\n",
    "#Make a function out of this one\n",
    "temp_list = []\n",
    "#def narrowing_search(ipn,trigger)\n",
    "for i in ipn_dt:\n",
    "    year,month,day,hour,minute = extract_datetime(i)\n",
    "    for row in trig_dt[0]:\n",
    "        if row.year == year and row.month == month and row.day == day and row.hour == hour and row.minute == minute:\n",
    "            temp_list.append(row)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "minute_match = np.array(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-catholic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-loading",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
